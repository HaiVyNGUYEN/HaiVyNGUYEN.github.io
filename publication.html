<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>Hai-Vy NGUYEN - Experience</title>
  <style>
    html {
      overflow-y: scroll;
    }
    body {
      font-family: sans-serif;
      margin: 0;
      padding: 0;
      line-height: 1.6;
    }
    header {
      position: sticky;
      top: 0;
      display: flex;
      align-items: center;
      padding: 1rem 3vw;
      border-bottom: 1px solid #ddd;
      background-color: #f9f9f9;
      z-index: 1000;
      max-width: 75vw;
      margin: auto;
    }
    header img {
      height: 140px;
      width: 140px;
      border-radius: 50%;
      margin-right: 1rem;
      object-fit: cover;
    }
    nav a {
      margin-right: 1rem;
      text-decoration: none;
      font-weight: bold;
      color: #007acc;
    }
main {
  padding: 2rem 0vw;
  max-width: 80vw;
  margin: auto;
  text-align: justify;
  hyphens: auto;
}
    h1, h2 {
      color: #2c3e50;
    }

      .experience-item {
      display: flex;
      align-items: flex-start;
      margin-bottom: 2rem;
    }
    .experience-item img {
      height: 350px;
      width: 350px;
      object-fit: contain;
      margin-right: 1rem;
    }
    
  </style>
</head>
<body>
  <header>
    <img src="vy_photo1.jpeg" alt="Hai-Vy Photo">
    <div>
      <h1 style="margin: 0;">Hai-Vy NGUYEN</h1>
      <nav style="margin-top: 0.5rem;">
        <a href="index.html">About Me</a>
        <a href="experience.html">Experience</a>
        <a href="publication.html">Publications</a>
      </nav>
    </div>
  </header>

  <main>
    <h2>Publications</h2>
    <p>
    <hr>
   <div class="experience-item">
      <img src="animations.gif" alt="Renault">
 <div>
    <strong>
    <a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/356e52580f8d14514eb5e7d2fa1696a0-Paper-Conference.pdf" 
       target="_blank" 
       style="color: darkgreen; text-decoration: underline;">
      Combining Statistical Depth and Fermat Distance for Uncertainty Quantification
    </a>
  </strong><br>
    <em><strong>Hai‑Vy Nguyen</strong>, Fabrice Gamboa, Reda Chhaibi, Sixin Zhang, Serge Gratton, Thierry Giaccone</em><br>
    <em>NeurIPS 2024 (Main Conference)</em>

    <p style="margin-top: 0.5rem;">
      This paper introduces a new framework for <strong>uncertainty quantification</strong> based on geometric principles in the feature space of neural networks.
      The method relies on two complementary ideas:
    </p>

    <ul style="margin-left: 1.2rem;">
      <li>
        <strong>Lens Depth</strong>: a statistical depth function that measures how “central” a test point is with respect to the training data distribution.
        It reflects how likely a point is to belong to a data manifold, based on its spatial relation to pairs of training points.
      </li>
      <li>
        <strong>Fermat Distance</strong>: a distance that favors high-density regions. It constructs a path that adapts to the data distribution and geometry.
      </li>
    </ul>

    <p>
      By combining these two quantities, we define an uncertainty score that quantifes the degree of "out-of-distribution'' (or centrality) with respect to a given distribution, taking into account its geometry and density.
      The score requires only the feature embeddings from a pre-trained model — no changes to training or retraining are needed.
      The proposed approach is <strong>non-parametric and model-agnostic</strong>.
      This work demonstrates that geometric insights in feature space can lead to powerful uncertainty quantification tools, suitable for real-world deployment.
    </p>
  </div>
    </div>
    <hr>
<h2>Preprints</h2>
<p>
<hr>
<div class="experience-item">
  <img src="combined.gif" alt="transfer">
  <div>
    <strong>
    <a href="https://arxiv.org/pdf/2505.06595" 
       target="_blank" 
       style="color: darkgreen; text-decoration: underline;">
      Feature Representation Transferring to Lightweight Models via <em>Perception Coherence
    </a>
    </strong><br>
    <em><strong>Hai‑Vy Nguyen</strong>, Fabrice Gamboa, Sixin Zhang, Reda Chhaibi, Serge Gratton, Thierry Giaccone</em><br>
    <em>Preprint, 2025</em>

    <p style="margin-top: 0.5rem;">
      We propose a novel framework for <strong>knowledge transfer</strong> from large teacher models to lightweight student models.
      Our approach introduces <strong>perception coherence</strong>, a probabilistic notion that ensures the student model mimics the teacher’s perception of relative similarities between inputs, without needing to exactly replicate feature geometries or distributions.
      This is particularly useful when student models have lower capacity or different feature dimensions than the teacher.
    </p>

    <ul style="margin-left: 1.2rem;">
      <li>
        <strong>Perception Coherence Loss:</strong> A ranking-based differentiable loss that encourages the student model to maintain the teacher's dissimilarity ordering among inputs.
      </li>
      <li>
        <strong>Flexible Representation Transfer:</strong> The method focuses on global coherence rather than exact feature matching, allowing more effective knowledge distillation for smaller models.
      </li>
    </ul>
  </div>
</div>
<hr>
    
<div class="experience-item">
  <img src="rec_attention.gif" alt="Rec_attention">
  <div>
    <strong>
    <a href="https://arxiv.org/pdf/2503.10875" 
       target="_blank" 
       style="color: darkgreen; text-decoration: underline;">
      Convolutional Rectangular Attention Module
    </a>
    </strong><br>
    <em><strong>Hai‑Vy Nguyen</strong>, Fabrice Gamboa, Sixin Zhang, Reda Chhaibi, Serge Gratton, Thierry Giaccone</em><br>
    <em>Preprint, 2024</em>

    <p style="margin-top: 0.5rem;">
      We introduce a novel <strong>spatial attention module</strong> for convolutional neural networks that guides the model to focus on the most discriminative regions of an input. 
      Unlike conventional position-wise attention maps, which often produce irregular boundaries and hamper generalization, our method constrains the attention region to a <strong>rectangular support</strong>, parametrized by only five parameters. 
      This design improves stability, generalization, and interpretability, enabling the model to learn where to look while training end-to-end.
    </p>

    <ul style="margin-left: 1.2rem;">
      <li>
        <strong>Rectangular Attention:</strong> Smooth rectangular attention supports that respect equivariance to translation, rotation, and scaling.
      </li>
      <li>
        <strong>Theoretical Insights:</strong> Analysis of how predicted attention maps align with ground-truth and improve feature separability in classification tasks.
      </li>
    </ul>

    <p>
      This module provides a simple yet effective attention mechanism that can be integrated into any CNN for improved performance and interpretability.
    </p>
  </div>
</div>
<hr>  

<div class="experience-item">
  <img src="loss_evolution.gif" alt="robust">
  <div>
    <strong>
    <a href="https://arxiv.org/pdf/2405.18499" 
       target="_blank" 
       style="color: darkgreen; text-decoration: underline;">
      Training More Robust Classification Model via Discriminative Loss and Gaussian Noise Injection
    </a>
    </strong><br>
    <em><strong>Hai‑Vy Nguyen</strong>, Fabrice Gamboa, Sixin Zhang, Reda Chhaibi, Serge Gratton, Thierry Giaccone</em><br>
    <em>Preprint, 2025</em>

    <p style="margin-top: 0.5rem;">
  We propose a novel <strong>training framework for noise-robust deep neural networks</strong> that addresses the trade-off between robustness and clean-data accuracy. 
  Our approach combines a <strong>penultimate-layer loss</strong> that enforces intra-class compactness and margin maximization with a <strong>class-wise feature alignment mechanism</strong> that aligns noisy and clean feature clusters. 
  Furthermore, we provide a <strong>theoretical analysis</strong> showing that stabilizing features under Gaussian noise reduces the curvature of the loss landscape, enhancing robustness without explicit curvature regularization.
</p>

<ul style="margin-left: 1.2rem;">
  <li>
    <strong>Penultimate-Layer Objective:</strong> Improves feature discriminativeness and class separability by enforcing compactness and margins to analytical decision boundaries.
  </li>
  <li>
    <strong>Class-Wise Feature Alignment:</strong> Brings noisy feature clusters closer to their clean counterparts, enhancing stability under input perturbations.
  </li>
  <li>
    <strong>Theoretical Guarantee:</strong> Demonstrates that reduced curvature (Hessian eigenvalues) in input space correlates with improved robustness.
  </li>
  <li>
    <strong>Robust Performance:</strong> Validated on standard benchmarks and a custom dataset, achieving robustness gains without sacrificing accuracy on clean data.
  </li>
</ul>
  </div>
</div>
<hr>  
    
  </main>
</body>
</html>
